{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data and build matrices\n",
    "train_df = pd.read_csv('data/train_potus_by_county.csv')\n",
    "X = train_df[train_df.columns[:-1]].as_matrix()\n",
    "y = train_df['Winner'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = open(\"performance.txt\", \"w\")\n",
    "log.write(\"We can test the performance of various untuned models using a 10-fold cross validation on the training data. Since we are classifying numerical features into two categories we explore the following models.\\n\")\n",
    "log.write('Testing performance of machine learning models: Decision Tree, Kernel SVM, Random Forest, kNN...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn import tree\n",
    "\n",
    "log.write('\\nRunning 10-fold cross validation with decision tree classifier.\\n')\n",
    "start = time.time()\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "scores = cross_val_score(estimator=clf,\n",
    "                X=X,\n",
    "                y=y,\n",
    "                n_jobs=-1,\n",
    "                cv=10,\n",
    "                verbose=1)\n",
    "log.write('Scores: ' + str(scores) + '\\n')\n",
    "log.write('Time taken: %fs\\n' % (time.time() - start))\n",
    "log.write('The decision tree classifier results in an average accuracy of %f%% with 10-fold cross validation.\\n' % (np.mean(scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM with RBF Kernel\n",
    "from sklearn import svm\n",
    "\n",
    "log.write('\\nRunning 10-fold cross validation with kernel SVM classifier.\\n')\n",
    "start = time.time()\n",
    "clf = svm.SVC(kernel='rbf', random_state=0)\n",
    "scores = cross_val_score(estimator = clf,\n",
    "                         X = X,\n",
    "                         y = y,\n",
    "                         n_jobs = -1,\n",
    "                         cv = 10,\n",
    "                         verbose = 1)\n",
    "log.write('Scores: ' + str(scores) + '\\n')\n",
    "log.write('Time taken: %fs\\n' % (time.time() - start))\n",
    "log.write('The rbf support vector machine classifier results in an average accuracy of %f%% with 10-fold cross validation.\\n' % (np.mean(scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "log.write('\\nRunning 10-fold cross validation with random forest classifier.\\n')\n",
    "start = time.time()\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "scores = cross_val_score(estimator = clf,\n",
    "                         X = X,\n",
    "                         y = y,\n",
    "                         n_jobs = -1,\n",
    "                         cv = 10,\n",
    "                         verbose = 1)\n",
    "log.write('Scores: ' + str(scores) + '\\n')\n",
    "log.write('Time taken: %fs\\n' % (time.time() - start))\n",
    "log.write('The random forest classifier results in an average accuracy of %f%% with 10-fold cross validation.\\n' % (np.mean(scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "log.write('\\nRunning 10-fold cross validation with kNN classifier.\\n')\n",
    "start = time.time()\n",
    "clf = KNeighborsClassifier()\n",
    "scores = cross_val_score(estimator = clf,\n",
    "                         X = X,\n",
    "                         y = y,\n",
    "                         n_jobs = -1,\n",
    "                         cv = 10,\n",
    "                         verbose = 1)\n",
    "log.write('Scores: ' + str(scores) + '\\n')\n",
    "log.write('Time taken: %fs\\n' % (time.time() - start))\n",
    "log.write('The kNN classifier results in an average accuracy of %f%% with 10-fold cross validation.\\n' % (np.mean(scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We get the best results with random forest classifiers. Let's attempt to tune the hyperparameters.\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper method to return results\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            log.write(\"Model with rank: {0}\\n\".format(i))\n",
    "            log.write(\"Mean validation score: {0:.3f} (std: {1:.3f})\\n\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            log.write(\"Parameters: {0}\\n\".format(results['params'][candidate]))\n",
    "            log.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use grid search to fine tune parameters for a forest with small number of estimators\n",
    "\n",
    "log.write('\\nUsing grid search before feature engineering to find optimal hyperparameters.\\n')\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"oob_score\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time.time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "log.write(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\\n\"\n",
    "      % (time.time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see if we can improve this performance with some feature engineering\n",
    "#\n",
    "# Random forest algorithms are invariant to linear transformations of our data, so we can use our domain knowledge\n",
    "# to instead attempt to combine features to make them more relevant\n",
    "\n",
    "# Population growth is a multiplier on total population\n",
    "train_df['Population growth'] = train_df['Population growth'] * train_df['Total population']\n",
    "\n",
    "# Percentage of people with bachelors degree or higher can be quantified\n",
    "train_df['% BachelorsDeg or higher'] = train_df['Total population'] * train_df['% BachelorsDeg or higher']\n",
    "\n",
    "# Unemployment rate can be quanitified to number unemployed\n",
    "train_df['Unemployment rate'] = train_df['Unemployment rate'] * train_df['Total population']\n",
    "\n",
    "# Household growth is a multiplier on the number of households\n",
    "train_df['House hold growth'] = train_df['House hold growth'] * train_df['Total households']\n",
    "\n",
    "# Per capita income growth is a multiplier on per capita income\n",
    "train_df['Per capita income growth'] = train_df['Per capita income growth'] * train_df['Per capita income']\n",
    "\n",
    "# Recreate matrices\n",
    "X = train_df[train_df.columns[:-1]].as_matrix()\n",
    "y = train_df['Winner'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use grid search again to fine tune parameters on our new feature set\n",
    "\n",
    "log.write('\\nUsing grid search after feature engineering to find optimal hyperparameters.\\n')\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"oob_score\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time.time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "log.write(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\\n\"\n",
    "      % (time.time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   22.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we know the optimum model and hyperparameters, we can train the model and save it\n",
    "import pickle\n",
    "\n",
    "log.write('\\nTraining optimal model with tuned hyperparameters.\\n')\n",
    "start = time.time()\n",
    "clf = RandomForestClassifier(random_state=0,\n",
    "                             n_estimators=1000,\n",
    "                             min_samples_split=10,\n",
    "                             min_samples_leaf=1,\n",
    "                             max_features=3,\n",
    "                             criterion='entropy',\n",
    "                             oob_score=True,\n",
    "                             max_depth=None,\n",
    "                             verbose=1).fit(X, y)\n",
    "with open('model.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "log.write('Time taken: %fs\\n' % (time.time() - start))\n",
    "log.write('Model trained and stored in file model.pkl\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
